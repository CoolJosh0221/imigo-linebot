# vLLM Server Dockerfile for SEA-LION-7B
# Optimized for NVIDIA RTX 4090 (24GB VRAM)
FROM docker.io/nvidia/cuda:12.6.2-runtime-ubuntu24.04

# Set environment variables for venv
ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONUNBUFFERED=1 \
    VIRTUAL_ENV=/opt/venv \
    PATH="/opt/venv/bin:$PATH"

# Install system dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    python3 \
    python3-pip \
    python3-venv \
    git \
    wget \
    curl \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

# Create virtual environment
RUN python3 -m venv $VIRTUAL_ENV

# Install vLLM with CUDA 12.6 support (using venv's pip)
RUN pip install --upgrade pip && \
    pip install vllm --extra-index-url https://download.pytorch.org/whl/cu126

# Install additional dependencies
RUN pip install requests

# Create model cache directory
RUN mkdir -p /root/.cache/huggingface/hub

# Set working directory
WORKDIR /app

# Expose vLLM server port
EXPOSE 8001

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=180s --retries=3 \
    CMD curl -f http://localhost:8001/health || exit 1

# CMD automatically uses venv's python due to PATH
CMD ["python3", "-m", "vllm.entrypoints.openai.api_server", \
     "--model", "aisingapore/Llama-SEA-LION-v3.5-8B-R", \
     "--dtype", "bfloat16", \
     "--max-model-len", "16384", \
     "--gpu-memory-utilization", "0.80", \
     "--tensor-parallel-size", "1", \
     "--trust-remote-code", \
     "--enable-prefix-caching", \
     "--host", "0.0.0.0", \
     "--port", "8001"]