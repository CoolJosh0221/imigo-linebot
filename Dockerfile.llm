# vLLM Server Dockerfile for SEA-LION-7B
# Requires NVIDIA GPU with CUDA support
FROM nvidia/cuda:12.1.0-runtime-ubuntu22.04

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1

# Install Python and system dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    python3.11 \
    python3-pip \
    git \
    wget \
    && rm -rf /var/lib/apt/lists/*

# Set Python 3.11 as default
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1 && \
    update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1

# Upgrade pip
RUN pip3 install --no-cache-dir --upgrade pip

# Install vLLM and dependencies
RUN pip3 install --no-cache-dir \
    vllm==0.3.0 \
    torch==2.1.2 \
    transformers==4.36.2 \
    accelerate==0.25.0

# Create model cache directory
RUN mkdir -p /root/.cache/huggingface

# Set working directory
WORKDIR /app

# Expose vLLM server port
EXPOSE 8001

# Health check
HEALTHCHECK --interval=60s --timeout=10s --start-period=120s --retries=3 \
    CMD python3 -c "import requests; requests.get('http://localhost:8001/health', timeout=5)"

# Run vLLM server
# Model will be downloaded on first run if not cached
CMD ["python3", "-m", "vllm.entrypoints.openai.api_server", \
     "--model", "aisingapore/sealion7b-instruct", \
     "--dtype", "auto", \
     "--max-model-len", "4096", \
     "--host", "0.0.0.0", \
     "--port", "8001"]
